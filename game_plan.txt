Game plan (also found in {game_plan.txt}):{{{
FYI - You can find a current system summary and analysis in {analysis_summaries.txt} and a clear flow/pipeline in {image.png}!!!! PLEASE USE THIS FOR YOUR REFERENCE!!!

Break this down into seperate tasks using you judgment, the end project should be clean and organised. At a later stage we will work together at getting it up and running and debugging. 

I want to try to use and keep as much of the current systems as we can but, I would like to combine and adapt them to the following task below. Use the pieces of our current project where you can, add to them where you need to, refactor if its neccessary. Prioritize optimal and cleanest systems. I want the final project repo to be organised, modular, and I want the system to be clean, well structured, modular and configurable. For instance I really like the way the llm system is setup, with .env files, config files, and profiles/ args for different features etc. I dont want redundencies, I dont want sub optimal workflows. If parts of the code/logic are useful but arent going to be used for solviong the task below, then keep them, you dont need to finish them or make them work now, but leave them as place holders for future enhancements and features - if they interfere with the desired system (that accomplishes the task below), you can remove them from the project, just make sure to document it. Make sure we have all the required documents as well, create a root level readme, requirements, usage.md, .env.example, etc. files, also create readme and usage for individual components if you need to. At the end of the project please clean unused files or code. PLEASE ask me when you need clarification of things, and please create a running document which you update regularly for your reference and mine, which contains designs decisions made, things removed, things we might add later, etc.. Also please make sure all the documentations and project organisation is clear and easy to comprehend, if someone new comes to the project I want it to be easy to get to grips with everything including usage, examples of how to setup and run things is crucial. 


The TASK (Also Found in {the_task.txt}): {{{
I have a new task,a colleague of minec used ChatGPT to find companies similar to a dataset of companies. He went on ChatGPT and he found loads of companies that are supposed to be similar, and it's created a bunch of hallucinations, like a bunch of fake numbers. So I have a spreadsheet now, and it's got 4,000 entries around, and he says that the URLs are correct, the actual websites, but the numbers, some of them are not correct. So what I need to do is I need to compare the number to the website, I want you to act like an experienced, meticulously expert data scientist approaching this problem. I want to do this in the best possible way, most efficient and optimized, and I don't mind using new tools if you have good suggestions. 

One important note - the only numbers that are considered high value/ usuable are ones from Germany, Switzerland, austria. 
}}}




Here's my recommended strategy:{{{
Okay, this is a classic data enrichment and validation problem, and your existing systems provide a fantastic foundation. As an experienced data scientist, my approach would be systematic, emphasizing accuracy, efficiency, and providing actionable insights, not just corrected data.



**Overall Philosophy: A "Waterfall" or "Tiered" Approach**

We want to use the cheapest, fastest methods first, and escalate to more expensive/slower (but potentially more accurate or context-aware) methods only when necessary. This optimizes for both cost and speed.

**Phase 1: Preparation & Setup**

1.  **Data Ingestion & Structuring:**
    *   COMPLETEY SCRAP THE SEQUEL INTEGRATIONS and Load the spreadsheet (4,000 entries) into a `pandas` DataFrame. Let's assume columns like `CompanyName`, `GivenURL`, `GivenPhoneNumber`.
    *   Add new columns to track our progress and findings:
        *   `NormalizedGivenPhoneNumber` (E.164 format)
        *   `ScrapingStatus` (e.g., 'Success', 'HTTPError', 'Timeout', 'RobotsDisallowed')
        *   `ScrapedContentSnippet` (optional, for debugging, maybe first 1KB of text)
        *   `RegexExtractedNumbers` (list of E.164 numbers)
        *   `LLMExtractedNumbers` (list of E.164 numbers from LLM with context)
        *   `LLMContext` (JSON or relevant text from LLM)
        *   `BestMatchedPhoneNumbers` (the ones we select)
        *   `VerificationStatus` (e.g., 'Confirmed', 'Corrected', 'Unverified_NotFoundOnSite', 'Unverified_ScrapeFail', 'MultipleCandidates_NeedsReview')
        *   `ConfidenceScore` (Low, Medium, High)
        *   `Notes` (any anomalies or specific findings)
        *   `(perhaps a field that is a unique id we can include with each run? )


2.  **Phone Number Normalization:**
    *   Use your Python `phonenumbers` library to parse and normalize the `GivenPhoneNumber` into a canonical format (E.164 is best: `+14155552671`). Store this in `NormalizedGivenPhoneNumber`. Handle parsing errors gracefully (e.g., mark as 'InvalidFormat').

3.  **Environment Setup:**
    *   Ensure all necessary libraries are installed
    *   Set up API keys for Gemini securely (e.g., environment variables).
    *   Implement robust logging using Python's `logging` module to track operations, errors, and warnings.

**Phase 2: Data Processing Loop (Iterate through each company)**

For each row in your DataFrame:

1.  **Step 1: Web Scraping**
    *   **Target URLs:** Primarily the `GivenURL` (homepage).
        *   Combine the text content from these key pages (and the homepage).
    *   **Enhanced Scraping (Optional but Recommended):**
        *   From the homepage, try to identify and prioritize links like "Contact Us", "About Us", "Support". Scrape these pages as well, as they are more likely to contain accurate contact information. You might limit this to 1 level deep from the homepage.

    *   **Scraping Implementation:**

        *   Handle HTTP errors (4xx, 5xx) gracefully.

        *   Store scraping status. If scraping fails, mark for manual review or a retry later.

2.  **Step 2: Regex-Based Phone Number Extraction**
    *   Apply your regex and `phonenumbers` library-based system to the *cleaned text content* from the scraped page(s).
    *   Extract all potential phone numbers.
    *   Normalize each found number to E.164 format using `phonenumbers.parse(number_string, REGION_CODE)` (you might need to infer `REGION_CODE` based on company location if available, or try common ones like 'US', 'GB', etc., or leave it more general if the numbers are already in international format). The `phonenumbers` library is quite good at this.
    *   Store these unique, normalized numbers in `RegexExtractedNumbers`.

3.  **Step 3: Initial Comparison & Decision Point**
    *   If `NormalizedGivenPhoneNumber` is valid and present in `RegexExtractedNumbers`:
        *   `VerificationStatus` = 'Confirmed'
        *   `BestMatchedPhoneNumber` = `NormalizedGivenPhoneNumber`
        *   `ConfidenceScore` = 'High' (if found on a contact page) or 'Medium' (if found elsewhere on homepage).
        *   
    *   Else (given number not found by regex, or given number was invalid): Proceed to LLM.

4.  **Step 4: LLM-Based Phone Number Extraction (Google Gemini)**
    *   **Condition to Run:**
        *   always run it for a "second opinion" if budget/time allows, or if regex finds multiple numbers and it's unclear which is primary.
    *   **Should the website content fed to the llm contain raw html/raw webpage text, clean text or other (i'm not sure PLEASE ADVISE ME on this!!!!!!!!!)
    *   **Prompt Engineering for Gemini:** This is crucial. create this in a new prompt (no need to destroy the current prompts)
    *   **EXAMPLE:{{{
        *   Input: The cleaned text content from the scraped page(s).
        *   Instruction:
            ```
            "From the following website text, please extract all business phone numbers. For each number, provide:
            1. The phone number in E.164 international format.
            2. The type or context of the number (e.g., 'Main Line', 'Sales', 'Support', 'Fax', 'Customer Service', 'Headquarters').
            3. Your confidence that this is a primary contact number (e.g., 'High', 'Medium', 'Low').

            Prioritize clearly identifiable primary business contact numbers. Avoid numbers that look like product IDs, order numbers, or are not clearly business contact lines.
            Please provide the output as a JSON list of objects, where each object represents a found phone number with its details.

            Website Text:
            ---
            [Insert scraped text here]
            ---
            "
            ```
}}}
    *   **API Call & Parsing:**
        *   Send the request to Gemini. Handle API errors.
        *   Parse the JSON response.
        *   Normalize numbers from LLM output again using `phonenumbers` just to be absolutely sure they are in E.164.
        *   Store in `LLMExtractedNumbers` (perhaps as a list of dicts, including the context).
        *   Store the raw/parsed LLM context in `LLMContext`.

5.  **Step 5: Final Verification & Selection**
    *   **Consolidate all found numbers:** Combine unique numbers from `RegexExtractedNumbers` and `LLMExtractedNumbers`.
    *   **Decision Logic:**
        *   **If `NormalizedGivenPhoneNumber` is in the consolidated list:**
            *   `VerificationStatus` = 'Confirmed'
            *   `BestMatchedPhoneNumber` = `NormalizedGivenPhoneNumber`
            *   `ConfidenceScore` = 'High' (especially if LLM also flagged it with high confidence or good context like 'Main Line').
        *   **Else if the consolidated list is NOT empty (i.e., new numbers were found):**
            *   `VerificationStatus` = 'Corrected'
            *   `ConfidenceScore`: Based on LLM confidence and context. 'High' if LLM provided a clear 'Main Line' with high confidence. 'Medium' if multiple numbers or less clear context.
            *   **Selecting `BestMatchedPhoneNumber`:**
                *   Prioritize numbers marked by LLM as 'Main Line', 'Headquarters', or with 'High' confidence.
                *   If multiple such candidates, you might list them all or pick the first one and flag for review.
                *   If only regex numbers, and multiple exist, this is harder. You might prioritize numbers found on "Contact Us" pages or those appearing more frequently (though frequency is a weak heuristic). Flag as 'MultipleCandidates_NeedsReview'.
        *   **Else (consolidated list is empty and `NormalizedGivenPhoneNumber` wasn't found earlier):**
            *   `VerificationStatus` = 'Unverified_NotFoundOnSite'
            *   `ConfidenceScore` = 'Low'
            *   `BestMatchedPhoneNumber` = None
        *   **Handle scraping failures:** If `ScrapingStatus` indicated failure, `VerificationStatus` = `Unverified_ScrapeFail`.

**Phase 3: Output & Review**

1.  **Save Results:** Export the augmented DataFrame to a new spreadsheet (e.g., CSV or Excel).
2.  **Reporting & Analysis:**
    *   Calculate summary statistics:
        *   % Confirmed
        *   % Corrected
        *   % Unverified (and reasons: NotFoundOnSite, ScrapeFail)
        *   % Requiring Manual Review
    *   This gives your colleague a clear picture of the data quality improvement.
3.  **Manual Review Queue:**
    *   Filter the output for rows with `VerificationStatus` like 'MultipleCandidates_NeedsReview', 'Unverified_ScrapeFail', or where confidence is 'Low'.
    *   This creates a targeted list for human review, which is much more efficient than manually checking all 4,000.

**Key Considerations & Optimizations:**

*   **Cost Management (LLM):** The waterfall approach naturally helps. Only use Gemini when simpler methods are insufficient. You could also sample a few hundred records first to estimate costs and refine LLM usage.
*   **Rate Limiting & Politeness:** Implement delays (`time.sleep()`) between `requests` calls to avoid overwhelming websites or getting IP banned.
*   **Caching:** If you might re-run this, consider caching scraped web content (e.g., store URL -> content in a simple file-based cache or a lightweight DB like SQLite). This saves repeated scraping. (SIMILAR TO THE WAY WE CURRENTLY SAVE THE SCRAPER FILE- IN FACT WE SHOULD PROBABLY KEEP THE WAY WE STORE FILES LOCAL - PLEASE GIVE ME YOUR OPINION ON WHATS OPTIMAL)
*   **Error Handling:** Extremely important. Network issues, malformed HTML, unexpected API responses â€“ your script needs to be resilient.

*   **Region Codes for `phonenumbers`:** If your dataset has country information for the companies, use that as the default `REGION` parameter when parsing phone numbers with the `phonenumbers` library. This improves parsing accuracy for numbers not in full E.164 format. If not, you might try a list of common regions or rely on the library's ability to guess (less reliable).
*   **Iterative Refinement:** Run a small batch (e.g., 50-100 companies) first. Analyze the results. Refine your prompts for Gemini, your regex, your scraping logic, etc., before processing all 4,000.

This is the url and numbers we will check found at {data_to_be_inputed.csv} 
}}}
}}}